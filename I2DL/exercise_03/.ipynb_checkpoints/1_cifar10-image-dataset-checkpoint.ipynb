{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR-10: Image Dataset\n",
    "\n",
    "Throughout this course, we will teach you all basic skills and supply you with all neccessary tools that you need to implement deep neural networks, which is the main focus of this class. However, you should also be proficient with handling data and know how to prepare it for your specific task. In fact, most of the jobs that involve deep learning in industry are very data related so this is an important skill that you have to pick up.\n",
    "\n",
    "Therefore, we we will take a deep dive into data preparation this week by implementing our own datasets and dataloader. In this notebook, we will focus on the image dataset CIFAR-10. The CIFAR-10 dataset consists of 50000 32x32 colour images in 10 classes, which are *plane*, *car*, *bird*, *cat*, *deer*, *dog*, *frog*, *horse*, *ship*, *truck*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by importing some libraries that we will need along the way, as well as our code files that we will work on throughout this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from exercise_code.data import (\n",
    "    ImageFolderDataset,\n",
    "    RescaleTransform,\n",
    "    NormalizeTransform,\n",
    "    ComposeTransform,\n",
    "    compute_image_mean_and_std,\n",
    ")\n",
    "from exercise_code.tests import (\n",
    "    test_image_folder_dataset,\n",
    "    test_rescale_transform,\n",
    "    test_compute_image_mean_and_std,\n",
    "    test_len_dataset,\n",
    "    test_item_dataset,\n",
    "    test_transform_dataset,\n",
    "    save_pickle\n",
    ")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Download\n",
    "Let us get started by downloading our data. In `exercise_code/data/image_folder_dataset.py` you can find a class `ImageFolderDataset`, which you will have to complete throughout this notebook.\n",
    "\n",
    "This class automatically downloads the raw data for you. To do so, simply initialize the class as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the output dataset folder\n",
    "i2dl_exercises_path = os.path.dirname(os.path.abspath(os.getcwd()))\n",
    "cifar_root = os.path.join(i2dl_exercises_path, \"datasets\", \"cifar10\")\n",
    "\n",
    "# Init the dataset and display downloading information this one time\n",
    "dataset = ImageFolderDataset(\n",
    "    root=cifar_root,\n",
    "    force_download=False,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now be able to see the images in `i2dl_exercises/datasets/cifar10` in your file browser, which should contain one subfolder per class, each containing the respective images labeled `0001.png`, `0002.png`, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the dataset will only be downloaded the first time you initialize a dataset class. If, for some reason, your version of the dataset gets corrupted and you wish to redownload it, simply initialize the class with `force_download=True` in the download cell above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training any model you should *always* take a look at some samples of your dataset. This way you can make sure that the data input has worked as intended and you can get a feeling for the dataset. \n",
    "\n",
    "Let's load the CIFAR-10 data and visualize a subset of the images. To do so, we use `PIL.Image.open()` to open an image, and then `numpy.asarray()` to cast the image to an numpy array, which will have shape 32x32x3. We will load 7 images per class in this way, and then we use `matplotlib.pyplot` to visualize the images in a grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_as_numpy(image_path):\n",
    "    return np.asarray(Image.open(image_path), dtype=float)\n",
    "\n",
    "classes = [\n",
    "    'plane', 'car', 'bird', 'cat', 'deer',\n",
    "    'dog', 'frog', 'horse', 'ship', 'truck',\n",
    "]\n",
    "num_classes = len(classes)\n",
    "samples_per_class = 7\n",
    "for label, cls in enumerate(sorted(classes)):\n",
    "    for i in range(samples_per_class):\n",
    "        image_path = os.path.join(\n",
    "            cifar_root,\n",
    "            cls,\n",
    "            str(i+1).zfill(4) + \".png\"\n",
    "        )  # e.g. cifar10/plane/0001.png\n",
    "        image = np.asarray(Image.open(image_path))  # open image as numpy array\n",
    "        plt_idx = i * num_classes + label + 1  # calculate plot location in the grid\n",
    "        plt.subplot(samples_per_class, num_classes, plt_idx)\n",
    "        plt.imshow(image.astype('uint8'))\n",
    "        plt.axis('off')\n",
    "        if i == 0:\n",
    "            plt.title(cls)  # plot class names above columns\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ImageFolderDataset Implementation\n",
    "\n",
    "Loading images as we have done above is a bit cumbersome. Therefore, we will now write a custom **Dataset** class, which takes care of the loading for us. This is always the first thing you have to implement when starting a new deep learning project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Dataset Class\n",
    "\n",
    "The **Dataset** class is a wrapper that loads the data from a given filepath and returns a dictionary containing already prepared data, as we have done above. Datasets always need to have the following two methods implemented:\n",
    "- `__len__(self)` is a method that should simply calculate and return the number of images in the dataset. After it is implemented, we can simply call it with `len(dataset)`.\n",
    "- `__getitem__(self, index)` should return the image with the given index from your dataset. Implementing this will allow you to access your dataset like a list, i.e. you can then simply call `dataset[9]` to access the 10th image in the dataset.\n",
    "\n",
    "Generally, you will have to implement a different dataset for every project. However, we will provide you with base dataset classes for future projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 ImageFolderDataset Implementation\n",
    "\n",
    "Now it is your turn to implement such a dataset class for CIFAR-10. To do so, open `exercise_code/data/image_folder_dataset.py` and check the following three methods of `ImageFolderDataset`:\n",
    "- `make_dataset(directory, class_to_idx)` should load the prepared data from a given directory root (`directory`) into two lists (`images` and `labels`). `class_to_idx` is a dict mapping class (e.g. 'cat') to label (e.g. 1).\n",
    "- `__len__(self)` should calculate and return the number of images in your dataset.\n",
    "- `__getitem__(self, index)` should return the image with the given index from your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <h3>Task: Check Code</h3>\n",
    "    <p>Please read <code>make_dataset(directory, class_to_idx)</code> and make sure to familiarize with its output as you will need to interact with it for the following tasks. Additionally, it would be wise decision to get familiar with python's os library which will be of utmost importance for most datasets you will write in future projects. As it is not beginner friendly, we removed it for this exercise but it is an important skill for a DL practicer.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task: Implement</h3>\n",
    "    <p>Implement the <code>__len__(self)</code> method and test your implementation by running the following cell.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ImageFolderDataset(\n",
    "    root=cifar_root,\n",
    ")\n",
    "\n",
    "_ = test_len_dataset(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task: Implement</h3>\n",
    "    <p>Implement the <code>__getitem__(self, index)</code> method and test your implementation by running the following cell.\n",
    "    </p>\n",
    "    <p><b>Hint:</b> You may want to reuse parts of the 'Data Visualization' code above in your implementation of <code>__getitem__()</code>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ImageFolderDataset(\n",
    "    root=cifar_root,\n",
    ")\n",
    "\n",
    "_ = test_item_dataset(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Dataset Usage\n",
    "\n",
    "Now that we have implemented all required parts of the ImageFolderDataset, using the `__getitem__()` method, you can now access your dataset as conveniently like we would access a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_item = dataset[0]\n",
    "sample_image = sample_item[\"image\"]\n",
    "sample_label = sample_item[\"label\"]\n",
    "\n",
    "print('Sample image shape:', sample_image.shape)\n",
    "print('Sample label:', sample_label)\n",
    "print('Sample image first values:', sample_image[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the images are represented as uint8 values for each of our three RGB color channels. The data type and scale will be important later.\n",
    "\n",
    "As you implemented both `__len__()` and `__getitem__()`, you can even iterate over your dataset now with a simple for loop! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 0\n",
    "for sample in tqdm(dataset):\n",
    "    num_samples += 1\n",
    "    \n",
    "print(\"Number of samples:\", num_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Transforms and Image Preprocessing\n",
    "\n",
    "Before training machine learning models, we often need to preprocess our data. For image datasets, two commonly applied techniques are:\n",
    "1. Normalize all images so that each value is either in [-1, 1] or [0, 1] as well as to convert it to floating point numbers by doing so\n",
    "2. Compute the mean image and substract it from all images in the dataset\n",
    "\n",
    "These transform classes are callables, meaning that you will be able to simply use them as follows:\n",
    "\n",
    "```transform = Transform()\n",
    "images_transformed = transform(images)```\n",
    "\n",
    "We will realize this is our pipeline by defining so called transforms. Instead of globally applying them to our input data, we will apply those for each sample after loading in our `__getitem__` call of our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task: Implement</h3>\n",
    "    <p>Revisit the <code>__getitem__(self, index)</code> method in <code>exercise_code/data/image_folder_dataset.py</code>  such that it applies to <code>self.transform</code>. With that change we can simply define the transforms during dataset creation and those will be automatically applied for each <code>__getitem__</code> call. Make sure not to break it though ;).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ImageFolderDataset(\n",
    "    root=cifar_root,\n",
    ")\n",
    "\n",
    "_ = test_transform_dataset(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equipped with this change, you can now easily add those two preprocessing techniques for CIFAR-10, as we will do in the following by implementing the classes `RescaleTransform` and `NormalizeTransform` in `exercise_code/data/transforms.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Rescaling Images using RescaleTransform\n",
    "\n",
    "Let's start by implementing `RescaleTransform`. If you look at the `__init__()` method you will notice it has four arguments:\n",
    "* **out_range** is the range you wish to rescale your images to. E.g. if you want to scale your images to [-1, 1], you would use `range=(-1, 1)`. By default, we will scale to [0, 1].\n",
    "* **in_range** is the value range of the data prior to rescaling. For uint8 images, this will always be (0, 255)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task: Implement</h3>\n",
    "    <p>Implement the <code>__call__()</code> method of <code>RescaleTransform</code> and test your implementation by running the following cell.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rescale_transform = RescaleTransform()\n",
    "dataset_rescaled = ImageFolderDataset(\n",
    "    root=cifar_root,\n",
    "    transform=rescale_transform\n",
    ")\n",
    "\n",
    "_ = test_rescale_transform(dataset_rescaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look at the first image, you should now see that all values are between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_item = dataset_rescaled[0]\n",
    "sample_label = sample_item[\"label\"]\n",
    "sample_image = sample_item[\"image\"]\n",
    "\n",
    "print(\"Max value:\", np.max(sample_image))\n",
    "print(\"Min value:\", np.min(sample_image))\n",
    "print('Sample rescaled image first values:', sample_image[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Normalize Images to Standard Gaussian using NormalizeTransform\n",
    "\n",
    "Let us now move on to the `NormalizeTransform` class. The `NormalizeTransform` class normalizes images channel-wise and it's `__init__` method has two arguments:\n",
    "* **mean** is the normalization mean, which we substract from the dataset.\n",
    "* **std** is the normalization standard deviation. By scaling our data with a factor of `1/std` we will normalize the standard deviation accordingly.\n",
    "\n",
    "Have a look at the code in `exercise_code/data/transforms.py`.\n",
    "\n",
    "What we would like to do now is to normalize our CIFAR-10 **images channel-wise** to standard normal. To do so, we need to calculate the **per-channel image mean and standard deviation first**, which we can then provide to `NormalizeTransform` to normalize our data accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first have to load all rescaled images\n",
    "rescaled_images = []\n",
    "for sample in tqdm(dataset_rescaled):\n",
    "    rescaled_images.append(sample[\"image\"])\n",
    "rescaled_images = np.array(rescaled_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task: Implement</h3>\n",
    "    <p>Implement the <code>compute_image_mean_and_std()</code> method in <code>exercise_code/data/transforms.py</code> and compute the rescaled dataset's mean and variance by running the following cell.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_mean, cifar_std = compute_image_mean_and_std(rescaled_images)\n",
    "print(\"Mean:\\t\", cifar_mean, \"\\nStd:\\t\", cifar_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test your implementation, run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = test_compute_image_mean_and_std(cifar_mean, cifar_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will delete the rescaled images now from your ram as they are no longer needed\n",
    "try:\n",
    "    del rescaled_images\n",
    "except NameError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the mean and standard deviation you computed to normalize the data we load, simply by adding the `NormalizeTransform` to the list of transformation our dataset applies in `__getitem__()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <h3>Task: Check Code</h3>\n",
    "    <p>Please check out the <code>ComposeTransform</code> in <code>transforms.py</code>. Later on, we will most often use multiple transforms and chain them together. Remember that the order is important here!</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up both transforms using the parameters computed above\n",
    "rescale_transform = RescaleTransform()\n",
    "normalize_transform = NormalizeTransform(\n",
    "    mean=cifar_mean,\n",
    "    std=cifar_std\n",
    ")\n",
    "\n",
    "final_dataset = ImageFolderDataset(\n",
    "    root=cifar_root,\n",
    "    transform=ComposeTransform([rescale_transform, normalize_transform])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now check out the results of our transformed samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_item = final_dataset[0]\n",
    "sample_label = sample_item[\"label\"]\n",
    "sample_image = sample_item[\"image\"]\n",
    "\n",
    "print('Sample normalized image shape:', sample_image.shape)\n",
    "print('Sample normalized image first values:', sample_image[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save your Dataset\n",
    "Now save your dataset and transforms using the following cell. This will save it to a pickle file `models/cifar_dataset.p`. We will use this dataset for the next notebook and it will count for the submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "    <h3>Note</h3>\n",
    "    <p>Each time you make changes in `dataset`, you need to rerun the following code to make your changes saved, but <b>this is NOT the file which you should submit</b>. You will find the final file for submission in the second notebook.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pickle(\n",
    "    data_dict={\n",
    "        \"dataset\": final_dataset,\n",
    "        \"cifar_mean\": cifar_mean,\n",
    "        \"cifar_std\": cifar_std,\n",
    "    },\n",
    "    file_name=\"cifar_dataset.p\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Takeaways\n",
    "1. Always have a look at your data before you start training any models on it.\n",
    "2. Datasets should be organized in corresponding **Dataset** classes that support `__len__` and `__getitem__` methods, which allows us to call `len(dataset)` and `dataset[index]`.\n",
    "3. Data often needs to be preprocessed; such preprocessing can be implemented in **Transform** classes, which are callables that can be simply applied via `data_transformed = transform(data)`. However, we will rarely do that and apply transforms on the fly using a dataloader which we introducing in the next notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
