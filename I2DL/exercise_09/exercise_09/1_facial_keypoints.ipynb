{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facial Keypoint Detection\n",
    "\n",
    "Welcome back for exercise 9! As we told you, the exercises of this lecture can be subdivided into mainly two parts. The first part in which we re-invented the wheel and implemented the most important methods on our own and the second part where we start using existing libraries (that already have implemented all the methods) and start playing around with more complex network architectures. \n",
    "\n",
    "We entered stage two already before Christmas, but with the introduction of convolution neural networks this week, we are given a very powerful tool that we want to explore in this exercises. So let us start with this week's exercise, where we ask you to build a convolution neural network to perform facial keypoint detection. \n",
    "\n",
    "Before we start, let's take a look at some example images and corresponding facial keypoints:\n",
    "\n",
    "<img src='images/key_pts_example.png' width=70% height=70%/>\n",
    "\n",
    "The facial keypoints (also called facial landmarks) are the small magenta dots shown on each of the faces in the image above. These keypoints mark important areas of the face: the eyes, corners of the mouth, the nose, etc. and are relevant for a variety of computer vision tasks, such as face filters, emotion recognition, pose recognition, and more. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preparation\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from exercise_code.data.facial_keypoints_dataset import FacialKeypointsDataset\n",
    "from exercise_code.networks.keypoint_nn import (\n",
    "    DummyKeypointModel,\n",
    "    KeypointModel\n",
    ")\n",
    "from exercise_code.util import (\n",
    "    show_all_keypoints,\n",
    "    save_model,\n",
    ")\n",
    "from exercise_code.tests import test_keypoint_nn\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h3>Note: Google Colab</h3>\n",
    "    <p>\n",
    "In case you don't have a GPU, you can run this notebook on Google Colab where you can access a GPU for free, but, of course, you can also run this notebook on your CPU.\n",
    "         </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Visualize Data\n",
    "To load the data, we have already prepared a Pytorch Dataset class `FacialKeypointsDataset` for you. You can find it in `exercise_code/data/facial_keypoints_dataset.py`. Run the following cell to download the data and initialize your dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_url = 'https://vision.in.tum.de/webshare/g/i2dl/facial_keypoints.zip'\n",
    "i2dl_exercises_path = os.path.dirname(os.path.abspath(os.getcwd()))\n",
    "data_root = os.path.join(i2dl_exercises_path, \"datasets\", \"facial_keypoints\")\n",
    "train_dataset = FacialKeypointsDataset(\n",
    "    train=True,\n",
    "    transform=transforms.ToTensor(),\n",
    "    root=data_root,\n",
    "    download_url=download_url\n",
    ")\n",
    "val_dataset = FacialKeypointsDataset(\n",
    "    train=False,\n",
    "    transform=transforms.ToTensor(),\n",
    "    root=data_root,\n",
    ")\n",
    "print(\"Number of training samples:\", len(train_dataset))\n",
    "print(\"Number of validation samples:\", len(val_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each sample in our dataset is a dict `{\"image\": image, \"keypoints\": keypoints}`, where\n",
    " * `image` is a [0-1]-normalized gray-scale image of size 96x96, represented by a torch tensor of shape (CxHxW) with C=1, H=96, W=96\n",
    "    <img style=\"float: right;\" src='images/key_pts_expl.png' width=50% height=50%/>\n",
    " * `keypoints` is the list of K facial keypoints, stored in a torch tensor of shape (Kx2). We have K=15 keypoints that stand for:\n",
    "   * keypoints[0]: Center of the left eye\n",
    "   * keypoints[1]: Center of the right eye\n",
    "   * keypoints[2]: Left eye inner corner\n",
    "   * keypoints[3]: Left eye outer corner\n",
    "   * keypoints[4]: Right eye inner corner\n",
    "   * keypoints[5]: Right eye outer corner\n",
    "   * keypoints[6]: Left eyebrow inner end\n",
    "   * keypoints[7]: Left eyebrow outer end\n",
    "   * keypoints[8]: Right eyebrow inner end\n",
    "   * keypoints[9]: Right eyebrow outer end\n",
    "   * keypoints[10]: Nose tip\n",
    "   * keypoints[11]: Mouth left corner\n",
    "   * keypoints[12]: Mouth right corner\n",
    "   * keypoints[13]: Mouth center top lip\n",
    "   * keypoints[14]: Mouth center bottom lip\n",
    "   \n",
    "Each individual facial keypoint is represented by two coordinates (x,y) that specify the horizontal and vertical location of the keypoint respectively. All keypoint values are normalized to [-1,1], such that:\n",
    "   * (x=-1,y=-1) corresponds to the top left corner, \n",
    "   * (x=-1,y=1) to the bottom left corner,\n",
    "   * (x=1,y=-1) to the top right corner,\n",
    "   * (x=1,y=1) to the bottom right corner,\n",
    "   * and (x=0,y=0) to the center of the image.\n",
    "   \n",
    "      \n",
    "The data downloaded is already preprocessed and hence there is no need to apply transformations in order to prepare the data. Of course, feel free to apply trainings transformation to improve your performance such as flipping the trainings images, but don't forget that these transformations (data augmentation) are only applied to your trainings set. Your validation and test set remain untouched. Also, when applying transformations such as flipping, make sure the predicted coordinates of your keypoints will change accordingly.\n",
    "\n",
    "Let's have a look at the first training sample to get a better feeling for the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, keypoints = train_dataset[0][\"image\"], train_dataset[0][\"keypoints\"]\n",
    "print(\"Shape of the image:\", image.size())\n",
    "print(\"Smallest value in the image:\", torch.min(image))\n",
    "print(\"Largest value in the image:\", torch.max(image))\n",
    "print(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keypoints = train_dataset[0][\"keypoints\"]\n",
    "print(keypoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `exercise_code/util/vis_utils.py` we also provide you with a function `show_all_keypoints()` that takes in an image and keypoints and displays where the predicted keypoints are in the image. Let's use it to plot the first few images of our training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_keypoints(dataset, num_samples=3):\n",
    "    for i in range(num_samples):\n",
    "        image = dataset[i][\"image\"]\n",
    "        key_pts = dataset[i][\"keypoints\"]\n",
    "        show_all_keypoints(image, key_pts)\n",
    "show_keypoints(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Facial Keypoint Detection\n",
    "Your task is to define and train a model for facial keypoint detection.\n",
    "\n",
    "The facial keypoint detection task can be seen as a regression problem, where the goal is to predict 30 different values that correspond to the 15 facial keypoint locations. Thus, we need to build a network that gets a (1x96x96) image as input and predicts 30 continuous outputs between [-1,1].\n",
    "\n",
    "## Dummy Model\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <h3>Task: Check Code</h3>\n",
    "    <p>In <code>exercise_code/networks/keypoint_nn.py</code> we defined a naive <code>DummyKeypointModel</code>, which always predicts the keypoints of the first training image. Let's try it on a few images and visualize our predictions in red:\n",
    " </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_keypoint_predictions(model, dataset, num_samples=3):\n",
    "    for i in range(num_samples):\n",
    "        image = dataset[i][\"image\"]\n",
    "        key_pts = dataset[i][\"keypoints\"]\n",
    "        predicted_keypoints = torch.squeeze(model(image).detach()).view(15,2)\n",
    "        show_all_keypoints(image, key_pts, predicted_keypoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_model = DummyKeypointModel()\n",
    "show_keypoint_predictions(dummy_model, train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, the model predicts the first sample perfectly, but for the remaining samples the predictions are quite off.\n",
    "\n",
    "## Loss and Metrics\n",
    "\n",
    "To measure the quality of the model's predictions, we will use the mean squared error (https://en.wikipedia.org/wiki/Mean_squared_error), summed up over all 30 keypoint locations. In PyTorch, the mean squared error is defined in `torch.nn.MSELoss()`, and we can use it like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.MSELoss()\n",
    "for i in range(3):\n",
    "    image = train_dataset[i][\"image\"]\n",
    "    keypoints = train_dataset[i][\"keypoints\"]\n",
    "    predicted_keypoints = torch.squeeze(dummy_model(image)).view(15,2)\n",
    "    loss = loss_fn(keypoints, predicted_keypoints)\n",
    "    print(\"Loss on image %d:\" % i, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, our dummy model achieves a loss close to 0 on the first sample, but on all other samples the loss is quite high.\n",
    "\n",
    "To obtain an evaluation score (in the notebook and on the submission server), we will use the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataset):\n",
    "    model.eval()\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "    loss = 0\n",
    "    for batch in dataloader:\n",
    "        image, keypoints = batch[\"image\"], batch[\"keypoints\"]\n",
    "        predicted_keypoints = model(image).view(-1,15,2)\n",
    "        loss += criterion(\n",
    "            torch.squeeze(keypoints),\n",
    "            torch.squeeze(predicted_keypoints)\n",
    "        ).item()\n",
    "    return 1.0 / (2 * (loss/len(dataloader)))\n",
    "\n",
    "print(\"Score of the Dummy Model:\", evaluate_model(dummy_model, val_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To pass the assignment, you will need to achieve a score of at least 100**. As you can see, the score is calculated from the average loss, so **your average loss needs to be lower than 0.005**. Our dummy model only gets a score of around 60, so you will have to come up with a better model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Design your own model\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task: Implement</h3>\n",
    "    <p> Now it is your turn to build your own model. To do so, you need to design a convolution neural network that takes images of size (Nx1x96x96) as input and produces outputs of shape (Nx30) in the range [-1,1]. Therefore, implement the <code>KeypointModel</code> class in <code>exercise_code/networks/keypoint_nn.py</code>.\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "Recall that CNN's are defined by a few types of layers:\n",
    "* Convolution layers\n",
    "* Max-pooling layers\n",
    "* Fully-connected layers\n",
    "\n",
    "You can design your network however you want, but we strongly suggest to include multiple convolution layers. You are also encouraged to use things like dropout and batch normalization to stabilize and regularize your network. If you want to build a really competitive model, have a look at some literature on keypoint detection, such as [this paper](https://arxiv.org/pdf/1710.00977.pdf).\n",
    "\n",
    "#### Define your model in the provided file \n",
    "`exercise_code/classifiers/keypoint_nn.py` file\n",
    "\n",
    "This file is mostly empty but contains the expected class name, and the methods that your model needs to implement (only `forward()` basically). You are also free to decide whether you want to use PyTorch Lightning or not.\n",
    "The only rules your model design has to follow are:\n",
    "* Inherit from either `torch.nn.Module` or `pytorch_lightning.LightningModule`\n",
    "* Perform the forward pass in forward(), predicting keypoints of shape (Nx30) for images of shape (Nx1x96x96)\n",
    "* Have less than 5 million parameters\n",
    "* Have a model size of less than 20MB after saving\n",
    "\n",
    "Furthermore, you need to pass all your hyperparameters to the model in a single dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    # TODO: if you have any model arguments/hparams, define them here\n",
    "}  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test whether your model follows the basic rules, run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeypointModel(hparams)\n",
    "test_keypoint_nn(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Train your model\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task: Implement</h3>\n",
    "    <p> In addition to the network itself, you will also need to write the code for the model training. You can use PyTorch Lightning for that, or you can also write it yourself in standard PyTorch.\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "**Hints:**\n",
    "* Use `torch.nn.MSELoss()` as loss function\n",
    "* Have a look at the previous notebooks for PyTorch and PyTorch Lightning in exercises 7 and 8 if you feel lost. In particular, revise `1_Cifar10_PytorchLightning.ipynb` in the optional submission from Week 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# TODO - Train Your Model                                              #\n",
    "########################################################################\n",
    "\n",
    "\n",
    "pass\n",
    "\n",
    "########################################################################\n",
    "#                           END OF YOUR CODE                           #\n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you're done training, run the cells below to visualize some predictions of your model, and to compute a validation score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_keypoint_predictions(model, val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Score:\", evaluate_model(model, val_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Save Your Model for Submission\n",
    "\n",
    "If your model achieved a validation score of 100 or higher, save your model with the cell below and submit it to [the submission server](https://dvl.in.tum.de/teaching/submission/). Your validation set is of course different from the test set on our server, so results may vary. Nevertheless, you will have a reasonable close approximation about your performance.\n",
    "\n",
    "Before that, we will check again whether the number of parameters is below 5 Mio. and the file size is below 20 MB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model, \"facial_keypoints.p\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congrats - you've now finished your first Convolution Neural Network! Simply run the following cell to create a zipped file for your implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now zip the folder for upload\n",
    "from exercise_code.util.submit import submit_exercise\n",
    "\n",
    "submit_exercise('exercise09')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission Goals\n",
    "\n",
    "- Goal: Implement and train a convolution neural network for facial keypoint detection.\n",
    "- Passing Criteria: Reach **Score >= 100** on __our__ test dataset. The submission system will show you your score after you submit.\n",
    "\n",
    "- Submission start: __Thursday, January 21, 2021 - 13:00__\n",
    "- Submission deadline: __Wednesday, January 27, 2021 - 15:59__ \n",
    "- You can make **$\\infty$** submissions until the deadline. Your __best submission__ will be considered for bonus"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
